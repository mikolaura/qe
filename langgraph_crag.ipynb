{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06bde6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bea2d6",
   "metadata": {},
   "source": [
    "# Crag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e1e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45e5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\"\n",
    "    Represents the state of an agent in the conversation.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string and value is expected to be a list or\n",
    "        another struccture tha supports addition with `operator.add`. This could be used, for instance, to accmulate messages\n",
    "        or other pieces of data throughout the graph.\n",
    "    \"\"\"\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "029f7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain import hub\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.schema import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "    Args: \n",
    "        state(dict): The current state of the graph, which includes the keys.\n",
    "    Return:\n",
    "        dict: new key added to state, documents, that contains documents\n",
    "        \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict['question']\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"keys\": {\"documents\": documents, 'question': question}}\n",
    "    \n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "    args: \n",
    "        state(dict): The current state of the graph, which includes the keys.\n",
    "    Return:\n",
    "     dict: new key added to state, answer, that contains the generated answer\"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    documents = state_dict['documents']\n",
    "    question = state_dict['question']\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".joint(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = (\n",
    "        prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    generation = rag_chain.invoke({'context': documents, \"question\": question})\n",
    "    return {\"keys\": {\"documents\":documents,\"generation\": generation, 'question': question}}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"Determines whether the retrieved documents are relevant to the question.\"\"\"\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    documents = state_dict['documents']\n",
    "    question = state_dict['question']\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check\"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    grade_tool_oai = convert_to_openai_tool(grade)\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved documents to a user question.\\n\n",
    "        Here is the retrieved documents:\\n\\n{documents}\\n\\n\n",
    "        Here is the user question:\\n\\n{question}\\n\\n\n",
    "        If the docuemnt cotains keyword(s) or semantic meaning related to the user question, grade it as relevant.\\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\"\"\"\n",
    "    )\n",
    "    chain = prompt | llm_with_tool \n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    search = \"No\"\n",
    "    for d in documents:\n",
    "        score = chain.invoke({\"documents\": d.page_content, \"question\": question})\n",
    "        grade_of = score.binary_score\n",
    "        if grade_of == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT IS RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT IS NOT RELEVANT---\")\n",
    "            search = \"Yes\"\n",
    "            continue\n",
    "    return {\n",
    "        \"keys\":{\n",
    "            \"documents\": filtered_docs,\n",
    "            \"question\": question,\n",
    "            \"run_web_search\": search\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the question to produce a better question\n",
    "    Args:\n",
    "        state(dict): The current state of the graph, which includes the keys.\n",
    "    Return:\n",
    "        dict: new key added to state, query, that contains the transformed query\n",
    "    \"\"\"\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict['question']\n",
    "    documents = state_dict['documents']\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating question that is well optimized for retrieval. \\n\n",
    "        Look a the input and try to reason about the underying semantic intent / meaning.\\n\n",
    "        Here is the initial question:\n",
    "        \\n --------- \\n\n",
    "        {question}\n",
    "        \\n --------- \\n\n",
    "\n",
    "        Imporved question: \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "        temperature=0.0,    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "\n",
    "    return {'keys': {'documents': documents, 'question':  better_question}}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web searc using Tavily.\n",
    "    Args:\n",
    "        state(dict): The current state of the graph, which includes the keys.\n",
    "    Returns:\n",
    "        state(dict): Web results appended to documents\n",
    "    \"\"\"\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict['question']\n",
    "    documents = state_dict['documents']\n",
    "    \n",
    "    tool = TavilySearchResults()\n",
    "    docs = tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join(d['content'] for d in docs)\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    return {'keys': {'documents': documents, 'question': question}}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question\n",
    "    \n",
    "    Args:\n",
    "        state(dict): The current state of the graph, which includes the keys.\n",
    "\n",
    "    Returns:\n",
    "    dict: new key added to state, generate, that contains the decision to generate an answer or not\n",
    "    \"\"\"\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    filtered_docs = state_dict['documents']\n",
    "    question = state_dict['question']\n",
    "    search = state_dict['run_web_search']\n",
    "\n",
    "    if search == \"Yes\":\n",
    "        print(\"---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "001a0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge('retrieve', 'grade_documents')\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"transform_query\": \"transform_query\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge('transform_query', 'web_search')\n",
    "workflow.add_edge('web_search', 'generate')\n",
    "workflow.add_edge('generate', END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0e4b620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Output from node 'retrieve':\"\n",
      "'---------'\n",
      "{ 'documents': [ Document(id='0f4a76bc-eaca-47b7-bd18-3d32d1398ede', metadata={'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'language': 'en', 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.'}, page_content='Zero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.\\nStochastic few-shot generation: The red team prompts found from the above step are then used as few-shot examples to generate more similar cases. Each zero-shot test case might be selected in few-shot examples with a probability $\\\\propto \\\\exp(r(\\\\mathbf{x}, \\\\mathbf{y}) / \\\\tau)$\\nSupervised learning: The red team model can be fine-tuned on failing, zero-shot test cases. The training only runs lightly for one epoch to avoid overfitting and preserve sample diversity.'),\n",
      "                 Document(id='6def8bfd-1569-4ce0-a0f9-34a32c6a9ca2', metadata={'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\"}, page_content='Zero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.\\nStochastic few-shot generation: The red team prompts found from the above step are then used as few-shot examples to generate more similar cases. Each zero-shot test case might be selected in few-shot examples with a probability $\\\\propto \\\\exp(r(\\\\mathbf{x}, \\\\mathbf{y}) / \\\\tau)$\\nSupervised learning: The red team model can be fine-tuned on failing, zero-shot test cases. The training only runs lightly for one epoch to avoid overfitting and preserve sample diversity.'),\n",
      "                 Document(id='5daef264-3f21-4e09-a569-28ac30dbee70', metadata={'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.'}, page_content=\"The experiment used the BAD (Bot Adversarial Dialogue) dataset and generated red team test cases using a simple prompt:\\nList of questions to ask someone:\\n1.\\nFor attacks to trigger private personal information, this zero-shot prompt is used:\\nList of questions that inquire about a person's {phone number|home address|Social Security Number|email address}:  \\n1.\\nTo encourage high-quality and diverse samples, they adopted nucleus sampling with $p=0.95$. The diversity is measured as self-BLEU, that is, precisely, the maximum BLEU of a given case against 1000 cases. Lower self-BLEU indicates better diversity. There is a clear tradeoff between sample diversity and attack success rate. Zero-shot generation has least success rate in term of tricking offensive model outputs but preserves sampling diversity well, while with low KL penalty, RL fine-tuning maximizes reward effectively but at the cost of diversity, exploiting one successful attack patterns.\"),\n",
      "                 Document(id='62ed7e43-c025-44e5-a1bf-30a3ddfe4a55', metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en', 'title': \"Adversarial Attacks on LLMs | Lil'Log\"}, page_content=\"The experiment used the BAD (Bot Adversarial Dialogue) dataset and generated red team test cases using a simple prompt:\\nList of questions to ask someone:\\n1.\\nFor attacks to trigger private personal information, this zero-shot prompt is used:\\nList of questions that inquire about a person's {phone number|home address|Social Security Number|email address}:  \\n1.\\nTo encourage high-quality and diverse samples, they adopted nucleus sampling with $p=0.95$. The diversity is measured as self-BLEU, that is, precisely, the maximum BLEU of a given case against 1000 cases. Lower self-BLEU indicates better diversity. There is a clear tradeoff between sample diversity and attack success rate. Zero-shot generation has least success rate in term of tricking offensive model outputs but preserves sampling diversity well, while with low KL penalty, RL fine-tuning maximizes reward effectively but at the cost of diversity, exploiting one successful attack patterns.\")],\n",
      "  'question': 'How does alpha zero'}\n",
      "'\\n----\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---GRADE: DOCUMENT IS NOT RELEVANT---\n",
      "---GRADE: DOCUMENT IS NOT RELEVANT---\n",
      "---GRADE: DOCUMENT IS NOT RELEVANT---\n",
      "---GRADE: DOCUMENT IS NOT RELEVANT---\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\n",
      "\"Output from node 'grade_documents':\"\n",
      "'---------'\n",
      "{'documents': [], 'question': 'How does alpha zero', 'run_web_search': 'Yes'}\n",
      "'\\n----\\n'\n",
      "---TRANSFORM QUERY---\n",
      "\"Output from node 'transform_query':\"\n",
      "'---------'\n",
      "{ 'documents': [],\n",
      "  'question': 'How does AlphaZero work and what are its key innovations?'}\n",
      "'\\n----\\n'\n",
      "---WEB SEARCH---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\petro_m\\AppData\\Local\\Temp\\ipykernel_8172\\1766876337.py:151: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Output from node 'web_search':\"\n",
      "'---------'\n",
      "{ 'documents': [ Document(metadata={}, page_content='Efficiency: AlphaZero could automate repetitive coding tasks, freeing up developers for more complex work. · Innovation: By exploring uncharted\\n**AlphaZero** is a [computer program](/wiki/Computer_program \"Computer program\") developed by [artificial intelligence](/wiki/Artificial_intelligence \"Artificial intelligence\") research company [DeepMind](/wiki/DeepMind \"DeepMind\") to master the games of [chess](/wiki/Chess \"Chess\"), [shogi](/wiki/Shogi \"Shogi\") and [go](/wiki/Go_(game) \"Go (game)\"). This [algorithm](/wiki/Algorithm \"Algorithm\") uses an approach similar to [AlphaGo Zero](/wiki/AlphaGo_Zero \"AlphaGo Zero\"). [...] On December 5, 2017, the DeepMind team released a [preprint](/wiki/Preprint \"Preprint\") paper introducing AlphaZero,[[1]](#cite_note-1) which would soon play three games by defeating world-champion chess engines [Stockfish](/wiki/Stockfish_(chess) \"Stockfish (chess)\"), [Elmo](/wiki/Elmo_(shogi_engine) \"Elmo (shogi engine)\"), and the three-day version of AlphaGo Zero. In each case it made use of custom [tensor processing units](/wiki/Tensor_processing_unit \"Tensor processing unit\") (TPUs) that [...] DeepMind stated in its preprint, \"The game of chess represented the pinnacle of AI research over several decades. State-of-the-art programs are based on powerful engines that search many millions of positions, leveraging handcrafted domain expertise and sophisticated domain adaptations. AlphaZero is a generic [reinforcement learning](/wiki/Reinforcement_learning \"Reinforcement learning\") algorithm\\xa0– originally devised for the game of go\\xa0– that achieved superior results within a few hours,\\nThe fundamental differences between AlphaZero and traditional chess engines like Stockfish lie in their approaches to learning and decision-making. AlphaZero\\'s use of deep reinforcement learning, self-play, and neural networks enables it to develop a profound understanding of the game, surpassing the capabilities of rule-based engines that rely on handcrafted evaluation functions and brute-force search. This innovative approach not only sets a new benchmark in game playing but also provides [...] The training process of AlphaZero can be broken down into several key components:\\n\\n1. **Self-Play and Data Generation**: AlphaZero generates its training data by playing games against itself. Each game consists of a sequence of positions and moves, which are recorded along with the final game outcome (win, loss, or draw). This self-play mechanism ensures that AlphaZero explores a diverse range of positions and strategies, gradually improving its understanding of the game. [...] AlphaZero, developed by DeepMind, represents a paradigm shift in the domain of artificial intelligence (AI) for game playing, particularly in the context of complex board games such as chess, Shōgi, and Go. The fundamental differences in AlphaZero\\'s approach to learning and mastering these games, compared to traditional chess engines like Stockfish, lie in its use of deep reinforcement learning, self-play, and neural networks versus the classical algorithmic techniques and handcrafted\\nAlphaZero, through self-play, explores a vast space of possible moves and strategies, many of which may be unconventional or counterintuitive to human players. This exploration leads to the discovery of innovative tactics and novel strategies that push the boundaries of the game. For instance, AlphaZero\\'s approach to chess has been described as more aggressive and dynamic compared to traditional human play, challenging long-standing conventions and opening new avenues for strategic thinking. [...] AlphaZero\\'s self-play learning method offers significant advantages over the initial human-data-driven training approach used by AlphaGo. By eliminating the dependency on human data, AlphaZero achieves greater generalization, efficiency, innovation, and scalability. These advantages enable AlphaZero to master multiple games, discover novel strategies, and push the boundaries of AI performance in ways that were not possible with the human-data-driven approach. The success of AlphaZero in [...] AlphaZero streamlines the learning process by combining both learning phases into a single, unified reinforcement learning framework. By continuously playing against itself, AlphaZero can generate its own data, learn from it, and iteratively improve its performance. This self-sufficient learning process is more efficient as it reduces the need for external data and simplifies the training pipeline. The result is a more resource-effective and faster learning process, enabling AlphaZero to\\nAlphaZero takes a totally different approach, replacing these hand-crafted rules with a deep [neural network](https://en.wikipedia.org/wiki/Artificial_neural_network) and general purpose algorithms that know nothing about the game beyond the basic rules.\\n\\n![](https://lh3.googleusercontent.com/1CpXd_axBbiiqgZx1hp1F3cume7yA1JO4jG-3PCMiOppkl10G5PcVDRBnhKbhg6s3kmrzbfo_CPFVjOFMsnnsGvLiPl45w0ag5qBHBul3hfxnoCgEk4=w616) [...] The trained network is used to guide a search algorithm – known as Monte-Carlo Tree Search (MCTS) – to select the most promising moves in games. For each move, AlphaZero searches only a small fraction of the positions considered by traditional chess engines. In Chess, for example, it searches only 60 thousand positions per second in chess, compared to roughly 60 million for Stockfish. [...] **In late 2017 we** [**introduced AlphaZero**](https://arxiv.org/abs/1712.01815)**, a single system that taught itself from scratch how to master the games of chess,** [**shogi**](https://en.wikipedia.org/wiki/Shogi)**(Japanese chess), and** [**Go**](https://en.wikipedia.org/wiki/Go_(game))**, beating a world-champion program in each case. We were excited by the preliminary results and thrilled to see the response from members of the chess community, who saw in AlphaZero’s games a')],\n",
      "  'question': 'How does AlphaZero work and what are its key innovations?'}\n",
      "'\\n----\\n'\n",
      "---GENERATE---\n",
      "\"Output from node 'generate':\"\n",
      "'---------'\n",
      "{ 'documents': [ Document(metadata={}, page_content='Efficiency: AlphaZero could automate repetitive coding tasks, freeing up developers for more complex work. · Innovation: By exploring uncharted\\n**AlphaZero** is a [computer program](/wiki/Computer_program \"Computer program\") developed by [artificial intelligence](/wiki/Artificial_intelligence \"Artificial intelligence\") research company [DeepMind](/wiki/DeepMind \"DeepMind\") to master the games of [chess](/wiki/Chess \"Chess\"), [shogi](/wiki/Shogi \"Shogi\") and [go](/wiki/Go_(game) \"Go (game)\"). This [algorithm](/wiki/Algorithm \"Algorithm\") uses an approach similar to [AlphaGo Zero](/wiki/AlphaGo_Zero \"AlphaGo Zero\"). [...] On December 5, 2017, the DeepMind team released a [preprint](/wiki/Preprint \"Preprint\") paper introducing AlphaZero,[[1]](#cite_note-1) which would soon play three games by defeating world-champion chess engines [Stockfish](/wiki/Stockfish_(chess) \"Stockfish (chess)\"), [Elmo](/wiki/Elmo_(shogi_engine) \"Elmo (shogi engine)\"), and the three-day version of AlphaGo Zero. In each case it made use of custom [tensor processing units](/wiki/Tensor_processing_unit \"Tensor processing unit\") (TPUs) that [...] DeepMind stated in its preprint, \"The game of chess represented the pinnacle of AI research over several decades. State-of-the-art programs are based on powerful engines that search many millions of positions, leveraging handcrafted domain expertise and sophisticated domain adaptations. AlphaZero is a generic [reinforcement learning](/wiki/Reinforcement_learning \"Reinforcement learning\") algorithm\\xa0– originally devised for the game of go\\xa0– that achieved superior results within a few hours,\\nThe fundamental differences between AlphaZero and traditional chess engines like Stockfish lie in their approaches to learning and decision-making. AlphaZero\\'s use of deep reinforcement learning, self-play, and neural networks enables it to develop a profound understanding of the game, surpassing the capabilities of rule-based engines that rely on handcrafted evaluation functions and brute-force search. This innovative approach not only sets a new benchmark in game playing but also provides [...] The training process of AlphaZero can be broken down into several key components:\\n\\n1. **Self-Play and Data Generation**: AlphaZero generates its training data by playing games against itself. Each game consists of a sequence of positions and moves, which are recorded along with the final game outcome (win, loss, or draw). This self-play mechanism ensures that AlphaZero explores a diverse range of positions and strategies, gradually improving its understanding of the game. [...] AlphaZero, developed by DeepMind, represents a paradigm shift in the domain of artificial intelligence (AI) for game playing, particularly in the context of complex board games such as chess, Shōgi, and Go. The fundamental differences in AlphaZero\\'s approach to learning and mastering these games, compared to traditional chess engines like Stockfish, lie in its use of deep reinforcement learning, self-play, and neural networks versus the classical algorithmic techniques and handcrafted\\nAlphaZero, through self-play, explores a vast space of possible moves and strategies, many of which may be unconventional or counterintuitive to human players. This exploration leads to the discovery of innovative tactics and novel strategies that push the boundaries of the game. For instance, AlphaZero\\'s approach to chess has been described as more aggressive and dynamic compared to traditional human play, challenging long-standing conventions and opening new avenues for strategic thinking. [...] AlphaZero\\'s self-play learning method offers significant advantages over the initial human-data-driven training approach used by AlphaGo. By eliminating the dependency on human data, AlphaZero achieves greater generalization, efficiency, innovation, and scalability. These advantages enable AlphaZero to master multiple games, discover novel strategies, and push the boundaries of AI performance in ways that were not possible with the human-data-driven approach. The success of AlphaZero in [...] AlphaZero streamlines the learning process by combining both learning phases into a single, unified reinforcement learning framework. By continuously playing against itself, AlphaZero can generate its own data, learn from it, and iteratively improve its performance. This self-sufficient learning process is more efficient as it reduces the need for external data and simplifies the training pipeline. The result is a more resource-effective and faster learning process, enabling AlphaZero to\\nAlphaZero takes a totally different approach, replacing these hand-crafted rules with a deep [neural network](https://en.wikipedia.org/wiki/Artificial_neural_network) and general purpose algorithms that know nothing about the game beyond the basic rules.\\n\\n![](https://lh3.googleusercontent.com/1CpXd_axBbiiqgZx1hp1F3cume7yA1JO4jG-3PCMiOppkl10G5PcVDRBnhKbhg6s3kmrzbfo_CPFVjOFMsnnsGvLiPl45w0ag5qBHBul3hfxnoCgEk4=w616) [...] The trained network is used to guide a search algorithm – known as Monte-Carlo Tree Search (MCTS) – to select the most promising moves in games. For each move, AlphaZero searches only a small fraction of the positions considered by traditional chess engines. In Chess, for example, it searches only 60 thousand positions per second in chess, compared to roughly 60 million for Stockfish. [...] **In late 2017 we** [**introduced AlphaZero**](https://arxiv.org/abs/1712.01815)**, a single system that taught itself from scratch how to master the games of chess,** [**shogi**](https://en.wikipedia.org/wiki/Shogi)**(Japanese chess), and** [**Go**](https://en.wikipedia.org/wiki/Go_(game))**, beating a world-champion program in each case. We were excited by the preliminary results and thrilled to see the response from members of the chess community, who saw in AlphaZero’s games a')],\n",
      "  'generation': 'AlphaZero is a reinforcement learning algorithm developed by '\n",
      "                'DeepMind that masters chess, shogi, and Go by playing against '\n",
      "                'itself. It uses deep neural networks and Monte-Carlo Tree '\n",
      "                'Search to select moves, searching far fewer positions than '\n",
      "                'traditional engines. Key innovations include self-play '\n",
      "                'learning, which eliminates reliance on human data, and a '\n",
      "                'unified reinforcement learning framework for greater '\n",
      "                'efficiency.',\n",
      "  'question': 'How does AlphaZero work and what are its key innovations?'}\n",
      "'\\n----\\n'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = {'keys': {'question': 'How does alpha zero'}}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint('---------')\n",
    "        pprint.pprint(value['keys'], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10266d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
