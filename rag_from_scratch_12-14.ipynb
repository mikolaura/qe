{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59cc1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babcb32",
   "metadata": {},
   "source": [
    "# Part 12: Multi-Representation indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557389c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194e38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "chain = (\n",
    "    {'doc': lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Sumarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "summaries = chain.batch(docs, {'max_concurrency': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d345ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This document is a comprehensive overview of LLM-powered autonomous agents. It outlines the key components of such systems: planning (task decomposition and self-reflection), memory (short-term and long-term), and tool use (leveraging external APIs). The document explores various techniques for each component, including Chain of Thought, Tree of Thoughts, ReAct, Reflexion, Chain of Hindsight, and Algorithm Distillation for planning; different types of memory and MIPS algorithms like LSH, ANNOY, HNSW, FAISS, and ScaNN for memory; and MRKL, TALM, Toolformer, HuggingGPT, and API-Bank for tool use. It also presents case studies like ChemCrow and Generative Agents to illustrate real-world applications. Finally, the document discusses the challenges facing LLM-powered agents, including finite context length, difficulties in long-term planning, and the reliability of natural language interfaces.',\n",
       " \"This Lil'Log post by Lilian Weng discusses the importance of high-quality human-annotated data for training modern deep learning models. It explores two main directions for achieving high data quality: the relationship between human raters and data quality, and the relationship between data quality and model training.\\n\\nRegarding human raters, the post covers task design, rater selection and training, and data aggregation techniques like majority voting, raw agreement, Cohen's Kappa, and probabilistic graph modeling (e.g., MACE) to identify and mitigate the impact of spammers. It also discusses two paradigms for data annotation: descriptive (embracing subjectivity) and prescriptive (enforcing consistency), highlighting the importance of understanding and accounting for diverse perspectives. Methods like disagreement deconvolution, multi-annotator models, and jury learning are presented as ways to capture and leverage systematic disagreement among annotators.\\n\\nRegarding data quality and model training, the post explores methods for identifying mislabeled data points based on training dynamics. Techniques discussed include influence functions, tracking prediction changes during training (e.g., Data Maps, forgetting events, Area Under the Margin (AUM)), and noisy cross-validation (NCV). These methods aim to identify and exclude potentially incorrect labels to improve model performance.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffbb9b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_chroma in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain_chroma) (0.5.23)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain_chroma) (0.115.6)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1.40 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain_chroma) (0.3.64)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain_chroma) (1.26.4)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.8.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.7.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.7.4)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.11.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.29.0)\n",
      "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.73.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10.6)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (13.9.2)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.41.3)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.3.45 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (0.3.45)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain_chroma) (3.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.37.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.9)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.1.40->langchain_chroma) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.1.40->langchain_chroma) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (24.12.23)\n",
      "Requirement already satisfied: protobuf in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.29.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.29.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.50b0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.50b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.17.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.20.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.24.5)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.3)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (14.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.3.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30171b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "\n",
    "store = InMemoryByteStore()\n",
    "id_key = 'doc_id'\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4) for _ in docs]\n",
    "\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b906aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '<function uuid4 at 0x0000029A8EA58400>'}, page_content='This document is a comprehensive overview of LLM-powered autonomous agents. It outlines the key components of such systems: planning (task decomposition and self-reflection), memory (short-term and long-term), and tool use (leveraging external APIs). The document explores various techniques for each component, including Chain of Thought, Tree of Thoughts, ReAct, Reflexion, Chain of Hindsight, and Algorithm Distillation for planning; different types of memory and MIPS algorithms like LSH, ANNOY, HNSW, FAISS, and ScaNN for memory; and MRKL, TALM, Toolformer, HuggingGPT, and API-Bank for tool use. It also presents case studies like ChemCrow and Generative Agents to illustrate real-world applications. Finally, the document discusses the challenges facing LLM-powered agents, including finite context length, difficulties in long-term planning, and the reliability of natural language interfaces.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query, k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49aa2f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thinking about High-Quality Human Data | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Thinking abo\n"
     ]
    }
   ],
   "source": [
    "retrived_docs = retriever.invoke(query, n_results=1)\n",
    "print(retrived_docs[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5e556",
   "metadata": {},
   "source": [
    "# Part 13: Raptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3808e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2c30605",
   "metadata": {},
   "source": [
    "# ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e24e7af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragatouille\n",
      "  Downloading ragatouille-0.0.9.post2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting llama-index (from ragatouille)\n",
      "  Downloading llama_index-0.12.41-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting faiss-cpu (from ragatouille)\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: langchain_core in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from ragatouille) (0.3.64)\n",
      "Collecting colbert-ai>=0.2.19 (from ragatouille)\n",
      "  Downloading colbert_ai-0.2.21-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: langchain in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from ragatouille) (0.3.13)\n",
      "Collecting onnx (from ragatouille)\n",
      "  Downloading onnx-1.18.0-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting srsly (from ragatouille)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting voyager (from ragatouille)\n",
      "  Downloading voyager-2.1.0-cp311-cp311-win_amd64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from ragatouille) (2.6.0+cu126)\n",
      "Collecting fast-pytorch-kmeans (from ragatouille)\n",
      "  Downloading fast_pytorch_kmeans-0.2.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from ragatouille) (3.3.1)\n",
      "Collecting bitarray (from colbert-ai>=0.2.19->ragatouille)\n",
      "  Downloading bitarray-3.4.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: datasets in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from colbert-ai>=0.2.19->ragatouille) (2.21.0)\n",
      "Collecting flask (from colbert-ai>=0.2.19->ragatouille)\n",
      "  Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting git-python (from colbert-ai>=0.2.19->ragatouille)\n",
      "  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from colbert-ai>=0.2.19->ragatouille) (1.0.1)\n",
      "Collecting ninja (from colbert-ai>=0.2.19->ragatouille)\n",
      "  Downloading ninja-1.11.1.4-py3-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from colbert-ai>=0.2.19->ragatouille) (1.14.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from colbert-ai>=0.2.19->ragatouille) (4.66.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from colbert-ai>=0.2.19->ragatouille) (4.44.0)\n",
      "Collecting ujson (from colbert-ai>=0.2.19->ragatouille)\n",
      "  Downloading ujson-5.10.0-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from torch>=1.13->ragatouille) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from torch>=1.13->ragatouille) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from torch>=1.13->ragatouille) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from torch>=1.13->ragatouille) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from torch>=1.13->ragatouille) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from torch>=1.13->ragatouille) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from sympy==1.13.1->torch>=1.13->ragatouille) (1.3.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from faiss-cpu->ragatouille) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from faiss-cpu->ragatouille) (24.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain->ragatouille) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain->ragatouille) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain->ragatouille) (3.9.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain->ragatouille) (0.3.4)\n",
      "Collecting langsmith<0.3,>=0.1.17 (from langchain->ragatouille)\n",
      "  Downloading langsmith-0.2.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain->ragatouille) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain->ragatouille) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain->ragatouille) (8.5.0)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_core (from ragatouille)\n",
      "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Downloading langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langchain_core->ragatouille) (1.33)\n",
      "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_agent_openai-0.4.10-py3-none-any.whl.metadata (439 bytes)\n",
      "Collecting llama-index-cli<0.5,>=0.4.2 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_cli-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting llama-index-core<0.13,>=0.12.41 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_core-0.12.41-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.6-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-llms-openai<0.5,>=0.4.0 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_llms_openai-0.4.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl.metadata (440 bytes)\n",
      "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
      "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_readers_file-0.4.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index->ragatouille)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index->ragatouille)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from onnx->ragatouille) (5.29.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from sentence-transformers->ragatouille) (1.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from sentence-transformers->ragatouille) (0.24.5)\n",
      "Requirement already satisfied: Pillow in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from sentence-transformers->ragatouille) (10.4.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.3 (from srsly->ragatouille)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (18.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille) (1.9.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain_core->ragatouille) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain->ragatouille) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain->ragatouille) (3.10.6)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain->ragatouille) (1.0.0)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille) (1.84.0)\n",
      "Collecting aiosqlite (from llama-index-core<0.13,>=0.12.41->llama-index->ragatouille)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.41->llama-index->ragatouille)\n",
      "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-core<0.13,>=0.12.41->llama-index->ragatouille) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-core<0.13,>=0.12.41->llama-index->ragatouille) (1.2.15)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.41->llama-index->ragatouille)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-core<0.13,>=0.12.41->llama-index->ragatouille) (1.2.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-core<0.13,>=0.12.41->llama-index->ragatouille) (1.6.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-core<0.13,>=0.12.41->llama-index->ragatouille) (0.7.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-core<0.13,>=0.12.41->llama-index->ragatouille) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-core<0.13,>=0.12.41->llama-index->ragatouille) (1.17.0)\n",
      "Collecting llama-cloud==0.1.26 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille)\n",
      "  Downloading llama_cloud-0.1.26-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-cloud==0.1.26->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille) (2024.7.4)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (4.12.3)\n",
      "Requirement already satisfied: pandas<2.3.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (2.2.2)\n",
      "Requirement already satisfied: pypdf<6,>=5.1.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (5.1.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille)\n",
      "  Downloading llama_parse-0.6.31-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from nltk>3.8.1->llama-index->ragatouille) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from nltk>3.8.1->llama-index->ragatouille) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from nltk>3.8.1->llama-index->ragatouille) (2024.7.24)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from requests<3,>=2->langchain->ragatouille) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from requests<3,>=2->langchain->ragatouille) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from requests<3,>=2->langchain->ragatouille) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragatouille) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from tqdm->colbert-ai>=0.2.19->ragatouille) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from transformers->colbert-ai>=0.2.19->ragatouille) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from transformers->colbert-ai>=0.2.19->ragatouille) (0.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (0.70.16)\n",
      "Collecting blinker>=1.9.0 (from flask->colbert-ai>=0.2.19->ragatouille)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from flask->colbert-ai>=0.2.19->ragatouille)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from flask->colbert-ai>=0.2.19->ragatouille) (2.1.3)\n",
      "Collecting werkzeug>=3.1.0 (from flask->colbert-ai>=0.2.19->ragatouille)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: gitpython in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from git-python->colbert-ai>=0.2.19->ragatouille) (3.1.43)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from scikit-learn->sentence-transformers->ragatouille) (3.5.0)\n",
      "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index->ragatouille)\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index->ragatouille) (3.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (2.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain->ragatouille) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain->ragatouille) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain->ragatouille) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain->ragatouille) (0.14.0)\n",
      "Collecting llama-cloud-services>=0.6.31 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille)\n",
      "  Downloading llama_cloud_services-0.6.31-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille) (0.8.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (2024.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.41->llama-index->ragatouille) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.41->llama-index->ragatouille) (3.23.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from gitpython->git-python->colbert-ai>=0.2.19->ragatouille) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai>=0.2.19->ragatouille) (5.0.1)\n",
      "Collecting platformdirs (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index->ragatouille)\n",
      "  Using cached platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\nlpreserch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (1.16.0)\n",
      "Downloading ragatouille-0.0.9.post2-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.1/46.1 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading colbert_ai-0.2.21-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.1/116.1 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading faiss_cpu-1.11.0-cp311-cp311-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/15.0 MB 8.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.8/15.0 MB 10.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.4/15.0 MB 10.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.8/15.0 MB 10.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.3/15.0 MB 11.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.9/15.0 MB 11.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.4/15.0 MB 11.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.9/15.0 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.5/15.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.1/15.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.1/15.0 MB 11.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.3/15.0 MB 9.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 6.0/15.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.2/15.0 MB 11.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 7.8/15.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 8.3/15.0 MB 11.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.9/15.0 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.4/15.0 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.0/15.0 MB 11.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.5/15.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.0/15.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.6/15.0 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.1/15.0 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.7/15.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.2/15.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.8/15.0 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.4/15.0 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.0/15.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.0/15.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.0/15.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.0/15.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.0/15.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 9.6 MB/s eta 0:00:00\n",
      "Downloading fast_pytorch_kmeans-0.2.2-py3-none-any.whl (9.8 kB)\n",
      "Downloading langchain_core-0.3.63-py3-none-any.whl (438 kB)\n",
      "   ---------------------------------------- 0.0/438.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 438.5/438.5 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading llama_index-0.12.41-py3-none-any.whl (7.1 kB)\n",
      "Downloading onnx-1.18.0-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.8 MB 15.9 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 1.0/15.8 MB 13.3 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.6/15.8 MB 12.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 2.2/15.8 MB 12.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.7/15.8 MB 12.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.2/15.8 MB 12.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.8/15.8 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 4.4/15.8 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.9/15.8 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.4/15.8 MB 12.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 6.0/15.8 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.4/15.8 MB 12.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 7.0/15.8 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 7.5/15.8 MB 12.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 8.1/15.8 MB 12.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.6/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.1/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.7/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.9/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.3/15.8 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.3/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.9/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.4/15.8 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.9/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.5/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.6/15.8 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 450.6/632.6 kB 9.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 632.6/632.6 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading voyager-2.1.0-cp311-cp311-win_amd64.whl (206 kB)\n",
      "   ---------------------------------------- 0.0/206.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 206.0/206.0 kB 12.2 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading langsmith-0.2.11-py3-none-any.whl (326 kB)\n",
      "   ---------------------------------------- 0.0/326.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 326.9/326.9 kB 10.2 MB/s eta 0:00:00\n",
      "Downloading llama_index_agent_openai-0.4.10-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_cli-0.4.3-py3-none-any.whl (28 kB)\n",
      "Downloading llama_index_core-0.12.41-py3-none-any.whl (7.7 MB)\n",
      "   ---------------------------------------- 0.0/7.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.6/7.7 MB 11.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.2/7.7 MB 12.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.7/7.7 MB 12.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.3/7.7 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.8/7.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.4/7.7 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.9/7.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.4/7.7 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.0/7.7 MB 11.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.6/7.7 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.1/7.7 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.7/7.7 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.2/7.7 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.7/7.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.7/7.7 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.7.6-py3-none-any.whl (16 kB)\n",
      "Downloading llama_cloud-0.1.26-py3-none-any.whl (266 kB)\n",
      "   ---------------------------------------- 0.0/266.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 266.8/266.8 kB 16.0 MB/s eta 0:00:00\n",
      "Downloading llama_index_llms_openai-0.4.5-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl (3.4 kB)\n",
      "Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
      "Downloading llama_index_readers_file-0.4.9-py3-none-any.whl (40 kB)\n",
      "   ---------------------------------------- 0.0/41.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.0/41.0 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading bitarray-3.4.2-cp311-cp311-win_amd64.whl (141 kB)\n",
      "   ---------------------------------------- 0.0/141.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 141.6/141.6 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 103.3/103.3 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading ninja-1.11.1.4-py3-none-win_amd64.whl (296 kB)\n",
      "   ---------------------------------------- 0.0/296.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 296.5/296.5 kB 9.2 MB/s eta 0:00:00\n",
      "Downloading ujson-5.10.0-cp311-cp311-win_amd64.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.1/42.1 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading llama_parse-0.6.31-py3-none-any.whl (4.9 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "   ---------------------------------------- 0.0/224.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 224.5/224.5 kB 13.4 MB/s eta 0:00:00\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading llama_cloud_services-0.6.31-py3-none-any.whl (38 kB)\n",
      "Using cached platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
      "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "   ---------------------------------------- 0.0/129.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 129.3/129.3 kB 7.9 MB/s eta 0:00:00\n",
      "Installing collected packages: striprtf, dirtyjson, bitarray, werkzeug, voyager, ujson, platformdirs, onnx, ninja, itsdangerous, griffe, faiss-cpu, catalogue, blinker, aiosqlite, srsly, nltk, flask, llama-cloud, langsmith, git-python, fast-pytorch-kmeans, banks, llama-index-core, langchain_core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, colbert-ai, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index, ragatouille\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 3.0.4\n",
      "    Uninstalling Werkzeug-3.0.4:\n",
      "      Successfully uninstalled Werkzeug-3.0.4\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.10.0\n",
      "    Uninstalling platformdirs-3.10.0:\n",
      "      Successfully uninstalled platformdirs-3.10.0\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.8.2\n",
      "    Uninstalling blinker-1.8.2:\n",
      "      Successfully uninstalled blinker-1.8.2\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.3.45\n",
      "    Uninstalling langsmith-0.3.45:\n",
      "      Successfully uninstalled langsmith-0.3.45\n",
      "  Attempting uninstall: langchain_core\n",
      "    Found existing installation: langchain-core 0.3.64\n",
      "    Uninstalling langchain-core-0.3.64:\n",
      "      Successfully uninstalled langchain-core-0.3.64\n",
      "Successfully installed aiosqlite-0.21.0 banks-2.1.2 bitarray-3.4.2 blinker-1.9.0 catalogue-2.0.10 colbert-ai-0.2.21 dirtyjson-1.0.8 faiss-cpu-1.11.0 fast-pytorch-kmeans-0.2.2 flask-3.1.1 git-python-1.0.3 griffe-1.7.3 itsdangerous-2.2.0 langchain_core-0.3.63 langsmith-0.2.11 llama-cloud-0.1.26 llama-cloud-services-0.6.31 llama-index-0.12.41 llama-index-agent-openai-0.4.10 llama-index-cli-0.4.3 llama-index-core-0.12.41 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.7.6 llama-index-llms-openai-0.4.5 llama-index-multi-modal-llms-openai-0.5.1 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.9 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.31 ninja-1.11.1.4 nltk-3.9.1 onnx-1.18.0 platformdirs-4.2.2 ragatouille-0.0.9.post2 srsly-2.5.1 striprtf-0.0.26 ujson-5.10.0 voyager-2.1.0 werkzeug-3.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60282c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd8153e706245f6a0167ccc6b47cd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--colbert-ir--colbertv2.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aac69f36e6a45adaa804cdb5ad9295d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d13db2918f9474fa3a286d894d335c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183aa74abfee4a578e611ad76049cffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c81cc7dad547c09043a8ac516d68c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb77e87f429f470ea377331892816b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75d650f83ea49c7b86fc27a89b3f05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\utils\\amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "RAG = RAGPretrainedModel.from_pretrained('colbert-ir/colbertv2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab6dc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9845574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Jun 11, 12:20:29] #> Creating directory .ragatouille/colbert\\indexes/Miyazali-123 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 11, 12:20:33] [0] \t\t #> Encoding 218 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\utils\\amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\utils\\amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 11, 12:20:47] [0] \t\t avg_doclen_est = 69.22935485839844 \t len(local_sample) = 218\n",
      "[Jun 11, 12:20:47] [0] \t\t Creating 1,024 partitions.\n",
      "[Jun 11, 12:20:47] [0] \t\t *Estimated* 15,091 embeddings.\n",
      "[Jun 11, 12:20:47] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert\\indexes/Miyazali-123\\plan.json ..\n",
      "used 20 iterations (73.9505s) to cluster 14338 items into 1024 clusters\n",
      "[Jun 11, 12:22:01] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:414: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "  warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch-based indexing did not succeed with error: Command '['where', 'cl']' returned non-zero exit status 1. ! Reverting to using FAISS and attempting again...\n",
      "________________________________________________________________________________\n",
      "WARNING! You have a GPU available, but only `faiss-cpu` is currently installed.\n",
      " This means that indexing will be slow. To make use of your GPU.\n",
      "Please install `faiss-gpu` by running:\n",
      "pip uninstall --y faiss-cpu & pip install faiss-gpu\n",
      " ________________________________________________________________________________\n",
      "Will continue with CPU indexing in 5 seconds...\n",
      "\n",
      "\n",
      "[Jun 11, 12:22:07] #> Note: Output directory .ragatouille/colbert\\indexes/Miyazali-123 already exists\n",
      "\n",
      "\n",
      "[Jun 11, 12:22:07] #> Will delete 1 files already at .ragatouille/colbert\\indexes/Miyazali-123 in 20 seconds...\n",
      "[Jun 11, 12:22:31] [0] \t\t #> Encoding 218 passages..\n",
      "[Jun 11, 12:22:39] [0] \t\t avg_doclen_est = 69.22935485839844 \t len(local_sample) = 218\n",
      "[Jun 11, 12:22:39] [0] \t\t Creating 1,024 partitions.\n",
      "[Jun 11, 12:22:39] [0] \t\t *Estimated* 15,091 embeddings.\n",
      "[Jun 11, 12:22:39] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert\\indexes/Miyazali-123\\plan.json ..\n",
      "[Jun 11, 12:22:40] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing decompress_residuals_cpp: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m RAG\u001b[38;5;241m.\u001b[39mindex(\n\u001b[0;32m      2\u001b[0m     collection\u001b[38;5;241m=\u001b[39m[full_document],\n\u001b[0;32m      3\u001b[0m     index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMiyazali-123\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     max_document_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      5\u001b[0m     split_documents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\ragatouille\\RAGPretrainedModel.py:211\u001b[0m, in \u001b[0;36mRAGPretrainedModel.index\u001b[1;34m(self, collection, document_ids, document_metadatas, index_name, overwrite_index, max_document_length, split_documents, document_splitter_fn, preprocessing_fn, bsize, use_faiss)\u001b[0m\n\u001b[0;32m    202\u001b[0m     document_splitter_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    203\u001b[0m collection, pid_docid_map, docid_metadata_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_corpus(\n\u001b[0;32m    204\u001b[0m     collection,\n\u001b[0;32m    205\u001b[0m     document_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m     max_document_length,\n\u001b[0;32m    210\u001b[0m )\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mindex(\n\u001b[0;32m    212\u001b[0m     collection,\n\u001b[0;32m    213\u001b[0m     pid_docid_map\u001b[38;5;241m=\u001b[39mpid_docid_map,\n\u001b[0;32m    214\u001b[0m     docid_metadata_map\u001b[38;5;241m=\u001b[39mdocid_metadata_map,\n\u001b[0;32m    215\u001b[0m     index_name\u001b[38;5;241m=\u001b[39mindex_name,\n\u001b[0;32m    216\u001b[0m     max_document_length\u001b[38;5;241m=\u001b[39mmax_document_length,\n\u001b[0;32m    217\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39moverwrite_index,\n\u001b[0;32m    218\u001b[0m     bsize\u001b[38;5;241m=\u001b[39mbsize,\n\u001b[0;32m    219\u001b[0m     use_faiss\u001b[38;5;241m=\u001b[39muse_faiss,\n\u001b[0;32m    220\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\ragatouille\\models\\colbert.py:342\u001b[0m, in \u001b[0;36mColBERT.index\u001b[1;34m(self, collection, pid_docid_map, docid_metadata_map, index_name, max_document_length, overwrite, bsize, use_faiss)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocid_pid_map[docid]\u001b[38;5;241m.\u001b[39mappend(pid)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocid_metadata_map \u001b[38;5;241m=\u001b[39m docid_metadata_map\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index \u001b[38;5;241m=\u001b[39m ModelIndexFactory\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint,\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection,\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_name,\n\u001b[0;32m    348\u001b[0m     overwrite,\n\u001b[0;32m    349\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    350\u001b[0m     bsize\u001b[38;5;241m=\u001b[39mbsize,\n\u001b[0;32m    351\u001b[0m     use_faiss\u001b[38;5;241m=\u001b[39muse_faiss,\n\u001b[0;32m    352\u001b[0m )\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_index_metadata()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\ragatouille\\models\\index.py:474\u001b[0m, in \u001b[0;36mModelIndexFactory.construct\u001b[1;34m(index_type, config, checkpoint, collection, index_name, overwrite, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;66;03m# NOTE: For now only PLAID indexes are supported.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m     index_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAID\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ModelIndexFactory\u001b[38;5;241m.\u001b[39m_MODEL_INDEX_BY_NAME[\n\u001b[0;32m    475\u001b[0m     ModelIndexFactory\u001b[38;5;241m.\u001b[39m_raise_if_invalid_index_type(index_type)\n\u001b[0;32m    476\u001b[0m ]\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m    477\u001b[0m     config, checkpoint, collection, index_name, overwrite, verbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    478\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\ragatouille\\models\\index.py:141\u001b[0m, in \u001b[0;36mPLAIDModelIndex.construct\u001b[1;34m(config, checkpoint, collection, index_name, overwrite, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct\u001b[39m(\n\u001b[0;32m    133\u001b[0m     config: ColBERTConfig,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAIDModelIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PLAIDModelIndex(config)\u001b[38;5;241m.\u001b[39mbuild(\n\u001b[0;32m    142\u001b[0m         checkpoint, collection, index_name, overwrite, verbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    143\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\ragatouille\\models\\index.py:243\u001b[0m, in \u001b[0;36mPLAIDModelIndex.build\u001b[1;34m(self, checkpoint, collection, index_name, overwrite, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m Indexer(\n\u001b[0;32m    238\u001b[0m         checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint,\n\u001b[0;32m    239\u001b[0m         config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[0;32m    240\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    241\u001b[0m     )\n\u001b[0;32m    242\u001b[0m     indexer\u001b[38;5;241m.\u001b[39mconfigure(avoid_fork_if_possible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 243\u001b[0m     indexer\u001b[38;5;241m.\u001b[39mindex(name\u001b[38;5;241m=\u001b[39mindex_name, collection\u001b[38;5;241m=\u001b[39mcollection, overwrite\u001b[38;5;241m=\u001b[39moverwrite)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\indexer.py:80\u001b[0m, in \u001b[0;36mIndexer.index\u001b[1;34m(self, name, collection, overwrite)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merase()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_does_not_exist \u001b[38;5;129;01mor\u001b[39;00m overwrite \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreuse\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__launch(collection)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_path\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\indexer.py:89\u001b[0m, in \u001b[0;36mIndexer.__launch\u001b[1;34m(self, collection)\u001b[0m\n\u001b[0;32m     87\u001b[0m     shared_queues \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     88\u001b[0m     shared_lists \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 89\u001b[0m     launcher\u001b[38;5;241m.\u001b[39mlaunch_without_fork(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, collection, shared_lists, shared_queues, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     93\u001b[0m manager \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mManager()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\infra\\launcher.py:93\u001b[0m, in \u001b[0;36mLauncher.launch_without_fork\u001b[1;34m(self, custom_config, *args)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (custom_config\u001b[38;5;241m.\u001b[39mavoid_fork_if_possible \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_config\u001b[38;5;241m.\u001b[39mavoid_fork_if_possible)\n\u001b[0;32m     92\u001b[0m new_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(custom_config)\u001b[38;5;241m.\u001b[39mfrom_existing(custom_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_config, RunConfig(rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 93\u001b[0m return_val \u001b[38;5;241m=\u001b[39m run_process_without_mp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallee, new_config, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m return_val\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\infra\\launcher.py:109\u001b[0m, in \u001b[0;36mrun_process_without_mp\u001b[1;34m(callee, config, *args)\u001b[0m\n\u001b[0;32m    106\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, config\u001b[38;5;241m.\u001b[39mgpus_[:config\u001b[38;5;241m.\u001b[39mnranks]))\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Run()\u001b[38;5;241m.\u001b[39mcontext(config, inherit_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 109\u001b[0m     return_val \u001b[38;5;241m=\u001b[39m callee(config, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    110\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_val\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\indexing\\collection_indexer.py:33\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(config, collection, shared_lists, shared_queues, verbose)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(config, collection, shared_lists, shared_queues, verbose: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     32\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m CollectionIndexer(config\u001b[38;5;241m=\u001b[39mconfig, collection\u001b[38;5;241m=\u001b[39mcollection, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m---> 33\u001b[0m     encoder\u001b[38;5;241m.\u001b[39mrun(shared_lists)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\indexing\\collection_indexer.py:68\u001b[0m, in \u001b[0;36mCollectionIndexer.run\u001b[1;34m(self, shared_lists)\u001b[0m\n\u001b[0;32m     65\u001b[0m print_memory_stats(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mresume \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaver\u001b[38;5;241m.\u001b[39mtry_load_codec():\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(shared_lists) \u001b[38;5;66;03m# Trains centroids from selected passages\u001b[39;00m\n\u001b[0;32m     69\u001b[0m distributed\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank)\n\u001b[0;32m     70\u001b[0m print_memory_stats(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\indexing\\collection_indexer.py:237\u001b[0m, in \u001b[0;36mCollectionIndexer.train\u001b[1;34m(self, shared_lists)\u001b[0m\n\u001b[0;32m    234\u001b[0m print_memory_stats(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m sample\n\u001b[1;32m--> 237\u001b[0m bucket_cutoffs, bucket_weights, avg_residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_avg_residual(centroids, heldout)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    240\u001b[0m     print_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_residual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_residual\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\indexing\\collection_indexer.py:315\u001b[0m, in \u001b[0;36mCollectionIndexer._compute_avg_residual\u001b[1;34m(self, centroids, heldout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_avg_residual\u001b[39m(\u001b[38;5;28mself\u001b[39m, centroids, heldout):\n\u001b[1;32m--> 315\u001b[0m     compressor \u001b[38;5;241m=\u001b[39m ResidualCodec(config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, centroids\u001b[38;5;241m=\u001b[39mcentroids, avg_residual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    317\u001b[0m     heldout_reconstruct \u001b[38;5;241m=\u001b[39m compressor\u001b[38;5;241m.\u001b[39mcompress_into_codes(heldout, out_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    318\u001b[0m     heldout_reconstruct \u001b[38;5;241m=\u001b[39m compressor\u001b[38;5;241m.\u001b[39mlookup_centroids(heldout_reconstruct, out_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\indexing\\codecs\\residual.py:24\u001b[0m, in \u001b[0;36mResidualCodec.__init__\u001b[1;34m(self, config, centroids, avg_residual, bucket_cutoffs, bucket_weights)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, centroids, avg_residual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bucket_cutoffs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bucket_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtotal_visible_gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 24\u001b[0m     ResidualCodec\u001b[38;5;241m.\u001b[39mtry_load_torch_extensions(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentroids \u001b[38;5;241m=\u001b[39m centroids\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mhalf()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\colbert\\indexing\\codecs\\residual.py:103\u001b[0m, in \u001b[0;36mResidualCodec.try_load_torch_extensions\u001b[1;34m(cls, use_gpu)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    102\u001b[0m print_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m decompress_residuals_cpp \u001b[38;5;241m=\u001b[39m load(\n\u001b[0;32m    104\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecompress_residuals_cpp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    105\u001b[0m     sources\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    106\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    107\u001b[0m             pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mresolve(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecompress_residuals.cpp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         ),\n\u001b[0;32m    109\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    110\u001b[0m             pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mresolve(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecompress_residuals.cu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         ),\n\u001b[0;32m    112\u001b[0m     ],\n\u001b[0;32m    113\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOLBERT_LOAD_TORCH_EXTENSION_VERBOSE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    114\u001b[0m )\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdecompress_residuals \u001b[38;5;241m=\u001b[39m decompress_residuals_cpp\u001b[38;5;241m.\u001b[39mdecompress_residuals_cpp\n\u001b[0;32m    117\u001b[0m print_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1380\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(name,\n\u001b[0;32m   1289\u001b[0m          sources: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   1290\u001b[0m          extra_cflags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1298\u001b[0m          is_standalone\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1299\u001b[0m          keep_intermediates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m    Load a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m        ...     verbose=True)\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _jit_compile(\n\u001b[0;32m   1381\u001b[0m         name,\n\u001b[0;32m   1382\u001b[0m         [sources] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sources, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m sources,\n\u001b[0;32m   1383\u001b[0m         extra_cflags,\n\u001b[0;32m   1384\u001b[0m         extra_cuda_cflags,\n\u001b[0;32m   1385\u001b[0m         extra_ldflags,\n\u001b[0;32m   1386\u001b[0m         extra_include_paths,\n\u001b[0;32m   1387\u001b[0m         build_directory \u001b[38;5;129;01mor\u001b[39;00m _get_build_directory(name, verbose),\n\u001b[0;32m   1388\u001b[0m         verbose,\n\u001b[0;32m   1389\u001b[0m         with_cuda,\n\u001b[0;32m   1390\u001b[0m         is_python_module,\n\u001b[0;32m   1391\u001b[0m         is_standalone,\n\u001b[0;32m   1392\u001b[0m         keep_intermediates\u001b[38;5;241m=\u001b[39mkeep_intermediates)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1823\u001b[0m, in \u001b[0;36m_jit_compile\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_standalone:\n\u001b[0;32m   1821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_exec_path(name, build_directory)\n\u001b[1;32m-> 1823\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _import_module_from_library(name, build_directory, is_python_module)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2245\u001b[0m, in \u001b[0;36m_import_module_from_library\u001b[1;34m(module_name, path, is_python_module)\u001b[0m\n\u001b[0;32m   2243\u001b[0m spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mspec_from_file_location(module_name, filepath)\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2245\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[0;32m   2246\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, importlib\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mLoader)\n\u001b[0;32m   2247\u001b[0m spec\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39mexec_module(module)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:573\u001b[0m, in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1233\u001b[0m, in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing decompress_residuals_cpp: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazali-123\",\n",
    "    max_document_length=100,\n",
    "    split_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d012f59",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m RAG\u001b[38;5;241m.\u001b[39msearch(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat animation studio did Miyazaki found?\u001b[39m\u001b[38;5;124m\"\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      2\u001b[0m result\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\ragatouille\\RAGPretrainedModel.py:315\u001b[0m, in \u001b[0;36mRAGPretrainedModel.search\u001b[1;34m(self, query, index_name, k, force_fast, zero_index_ranks, doc_ids, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    285\u001b[0m     query: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    292\u001b[0m ):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Query an index.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m \n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m    316\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[0;32m    317\u001b[0m         index_name\u001b[38;5;241m=\u001b[39mindex_name,\n\u001b[0;32m    318\u001b[0m         k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    319\u001b[0m         force_fast\u001b[38;5;241m=\u001b[39mforce_fast,\n\u001b[0;32m    320\u001b[0m         zero_index_ranks\u001b[38;5;241m=\u001b[39mzero_index_ranks,\n\u001b[0;32m    321\u001b[0m         doc_ids\u001b[38;5;241m=\u001b[39mdoc_ids,\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    323\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\ragatouille\\models\\colbert.py:393\u001b[0m, in \u001b[0;36mColBERT.search\u001b[1;34m(self, query, index_name, k, force_fast, zero_index_ranks, doc_ids)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# TODO We may want to load an existing index here instead;\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m#      For now require that either index() was called, or an existing one was loaded.\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    395\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m     force_fast\u001b[38;5;241m=\u001b[39mforce_fast,\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    408\u001b[0m to_return \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed70cb23",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m retriever \u001b[38;5;241m=\u001b[39m RAG\u001b[38;5;241m.\u001b[39mas_langchain_retriever(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m retriever\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat animation studio did Miyazaki found?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\langchain_core\\retrievers.py:259\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 259\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[0;32m    261\u001b[0m     )\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\ragatouille\\integrations\\_langchain.py:20\u001b[0m, in \u001b[0;36mRAGatouilleLangChainRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     15\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m     17\u001b[0m     run_manager: CallbackManagerForRetrieverRun,  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     18\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get documents relevant to a query.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msearch(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     22\u001b[0m         Document(\n\u001b[0;32m     23\u001b[0m             page_content\u001b[38;5;241m=\u001b[39mdoc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m], metadata\u001b[38;5;241m=\u001b[39mdoc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m     24\u001b[0m         )\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs\n\u001b[0;32m     26\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\ragatouille\\RAGPretrainedModel.py:315\u001b[0m, in \u001b[0;36mRAGPretrainedModel.search\u001b[1;34m(self, query, index_name, k, force_fast, zero_index_ranks, doc_ids, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    285\u001b[0m     query: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    292\u001b[0m ):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Query an index.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m \n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m    316\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[0;32m    317\u001b[0m         index_name\u001b[38;5;241m=\u001b[39mindex_name,\n\u001b[0;32m    318\u001b[0m         k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    319\u001b[0m         force_fast\u001b[38;5;241m=\u001b[39mforce_fast,\n\u001b[0;32m    320\u001b[0m         zero_index_ranks\u001b[38;5;241m=\u001b[39mzero_index_ranks,\n\u001b[0;32m    321\u001b[0m         doc_ids\u001b[38;5;241m=\u001b[39mdoc_ids,\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    323\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\nlpReserch\\Lib\\site-packages\\ragatouille\\models\\colbert.py:393\u001b[0m, in \u001b[0;36mColBERT.search\u001b[1;34m(self, query, index_name, k, force_fast, zero_index_ranks, doc_ids)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# TODO We may want to load an existing index here instead;\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m#      For now require that either index() was called, or an existing one was loaded.\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    395\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m     force_fast\u001b[38;5;241m=\u001b[39mforce_fast,\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    408\u001b[0m to_return \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fffb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpReserch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
