{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59bdea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c288047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1245299f",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4dcc2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller, more manageable steps. This can be achieved through prompting the model to \"think step by step\" or by using task-specific instructions. It can also be done with human inputs.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path=('https://lilianweng.github.io/posts/2023-06-23-agent/',),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    model_kwargs = {\n",
    "        \"seed\": 42\n",
    "    }\n",
    "    )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriver | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke('What is Task decomposition?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7426a0",
   "metadata": {},
   "source": [
    "# Part 2 Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20406a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8b947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string:str, encoding_name:str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f22446d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb0f810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8569384258805943\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1*norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af208bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_path=('https://lilianweng.github.io/posts/2023-06-23-agent/',),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8331ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2033ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "retriver = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59548653",
   "metadata": {},
   "source": [
    "# Part 3: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f6ca7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "\n",
    "retriver = vectorstore.as_retriever(search_kwargs={\"k\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9489da3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_7548\\4276928549.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriver.get_relevant_documents(\"What is task decomposition?\")\n"
     ]
    }
   ],
   "source": [
    "docs = retriver.get_relevant_documents(\"What is task decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "372dd6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c661c",
   "metadata": {},
   "source": [
    "# Part 4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83c4da4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the folowing context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the folowing context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e66c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33f22f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task decomposition is the process of breaking down a complicated task into smaller, simpler steps. It can be done by LLM with simple prompting, by using task-specific instructions, or with human inputs.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--fdd599fb-8e2c-4df1-992f-2b61fa22b3e6-0', usage_metadata={'input_tokens': 325, 'output_tokens': 41, 'total_tokens': 366, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\":docs, \"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16cc881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dd712e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hub_rag.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96d937d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down a complicated task into smaller and simpler steps. It can be done by LLM with simple prompting, by using task-specific instructions, or with human inputs.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriver, \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e42e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpReserch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
